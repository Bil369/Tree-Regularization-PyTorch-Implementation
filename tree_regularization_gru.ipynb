{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pydotplus\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# hyper-parameters about RNN model\n",
    "sequence_length = 8\n",
    "input_size = 8\n",
    "hidden_size = 20\n",
    "num_layers = 1\n",
    "num_classes = 10\n",
    "# hyper-parameters about optimizer\n",
    "learning_rate = 0.01\n",
    "# Hyper-parameters about training control\n",
    "batch_size = 32\n",
    "num_iters = 300\n",
    "iters_retrain = 20\n",
    "num_retrains = num_iters // iters_retrain\n",
    "lambda_punish = 0.1 # regularization strength about RNN\n",
    "epsilon_punish = 0.01 # regularization strength about surrogate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: tensor of shape (batch_size, seq_length, input_size)\n",
    "        # Set initial hidden and cell states \n",
    "        # h0: still tensor of shape (num_layers*num_directions, batch_size, hidden_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.gru(x, h0)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrogateModel(nn.Module):\n",
    "    \n",
    "    '''Fully connected neural network with one hidden layer\n",
    "       Split the fc1 into three parts \n",
    "       because only in this way can have compute graph with RNN model weights\n",
    "       so that can backpropagation to update RNN model weights and this is tree regularization\n",
    "       (maybe have other ways to do this faster. Currently this is not very elegant.)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SurrogateModel, self).__init__()\n",
    "        self.fc1_1 = nn.Linear(480, 20)\n",
    "        self.fc1_2 = nn.Linear(1200, 20)\n",
    "        self.fc1_3 = nn.Linear(200, 20)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is the model.state_dict().items()[training] or model.named_parameters()[calculate APL]\n",
    "        for key, value in x:\n",
    "            if key == 'gru.weight_ih_l0':\n",
    "                out1 = self.fc1_1(value.view(-1))\n",
    "            elif key == 'gru.weight_hh_l0':\n",
    "                out2 = self.fc1_2(value.view(-1))\n",
    "            elif key == 'fc.weight':\n",
    "                out3 = self.fc1_3(value.view(-1))\n",
    "        out = out1 + out2 + out3\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jth_minibatach(j, batch_size, X_train, y_train):\n",
    "    '''返回数据集中的第j个minibatch\n",
    "       \n",
    "       @param j: 第j次iters_retrain\n",
    "       @param batch_size: int\n",
    "       @param X_train: torch.tensor\n",
    "       @param y_train: torch.tensor\n",
    "    '''\n",
    "    num_data = y_train.size(0)\n",
    "    num_minibatches = num_data // batch_size + ((num_data % batch_size) > 0)\n",
    "    j = j % num_minibatches\n",
    "    start = j * batch_size\n",
    "    stop = start + batch_size\n",
    "    return X_train[start:stop], y_train[start:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_weights(model):\n",
    "    '''打印模型的各层weight参数个数\n",
    "    '''\n",
    "    for key, value in model.state_dict().items():\n",
    "        if key.endswith('weight'):\n",
    "            print(torch.prod(torch.tensor(value.size())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_APL_train(saved_model_state_dict, X_train):\n",
    "    tmp_model = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "    tmp_model.to(device)\n",
    "    y_APL_train = torch.zeros(len(saved_model_state_dict))\n",
    "    for i in range(len(saved_model_state_dict)):\n",
    "        tmp_model.load_state_dict(saved_model_state_dict[i])\n",
    "        X_train = X_train.view(-1, sequence_length, input_size)\n",
    "        X_train = X_train.to(device)\n",
    "        outputs = tmp_model(X_train)\n",
    "        _, y_pred = torch.max(outputs.data, 1)\n",
    "        tree = DecisionTreeClassifier(min_samples_leaf=25)\n",
    "        X_train = X_train.view(y_pred.size(0), -1)\n",
    "        X_train = X_train.to(torch.device('cpu'))\n",
    "        y_pred = y_pred.to(torch.device('cpu'))\n",
    "        tree.fit(X_train.numpy(), y_pred.numpy())\n",
    "        decision_path_matrix = tree.decision_path(X_train.numpy())\n",
    "        apl = decision_path_matrix.sum() / X_train.size(0)\n",
    "        y_APL_train[i] = apl\n",
    "    return y_APL_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "data = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.25, random_state=2020)\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float), torch.tensor(X_test, dtype=torch.float)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.long), torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gru.weight_ih_l0', torch.Size([60, 8])),\n",
       " ('gru.weight_hh_l0', torch.Size([60, 20])),\n",
       " ('gru.bias_ih_l0', torch.Size([60])),\n",
       " ('gru.bias_hh_l0', torch.Size([60])),\n",
       " ('fc.weight', torch.Size([10, 20])),\n",
       " ('fc.bias', torch.Size([10]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "[(param[0], param[1].size()) for param in list(model.named_parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN model......\n",
      "RNN iters: [10]/[300] loss: 2.01\n",
      "RNN iters: [20]/[300] loss: 1.66\n",
      "Accuracy of the network on the digits dataset: 52.67 %\n",
      "AUC of the network on the digits dataset: 0.90\n",
      "Training DNN model......\n",
      "RNN iters: [30]/[300] loss: 1.44\n",
      "RNN iters: [40]/[300] loss: 1.13\n",
      "Training DNN model......\n",
      "RNN iters: [50]/[300] loss: 1.06\n",
      "RNN iters: [60]/[300] loss: 0.81\n",
      "Training DNN model......\n",
      "RNN iters: [70]/[300] loss: 0.82\n",
      "RNN iters: [80]/[300] loss: 0.62\n",
      "Accuracy of the network on the digits dataset: 78.89 %\n",
      "AUC of the network on the digits dataset: 0.97\n",
      "Training DNN model......\n",
      "RNN iters: [90]/[300] loss: 0.66\n",
      "RNN iters: [100]/[300] loss: 0.56\n",
      "Training DNN model......\n",
      "RNN iters: [110]/[300] loss: 0.53\n",
      "RNN iters: [120]/[300] loss: 0.43\n",
      "Training DNN model......\n",
      "RNN iters: [130]/[300] loss: 0.42\n",
      "RNN iters: [140]/[300] loss: 0.32\n",
      "Accuracy of the network on the digits dataset: 85.78 %\n",
      "AUC of the network on the digits dataset: 0.98\n",
      "Training DNN model......\n",
      "RNN iters: [150]/[300] loss: 0.32\n",
      "RNN iters: [160]/[300] loss: 0.22\n",
      "Training DNN model......\n",
      "RNN iters: [170]/[300] loss: 0.21\n",
      "RNN iters: [180]/[300] loss: 0.16\n",
      "Training DNN model......\n",
      "RNN iters: [190]/[300] loss: 0.20\n",
      "RNN iters: [200]/[300] loss: 0.13\n",
      "Accuracy of the network on the digits dataset: 90.44 %\n",
      "AUC of the network on the digits dataset: 0.99\n",
      "Training DNN model......\n",
      "RNN iters: [210]/[300] loss: 0.16\n",
      "RNN iters: [220]/[300] loss: 0.12\n",
      "Training DNN model......\n",
      "RNN iters: [230]/[300] loss: 0.11\n",
      "RNN iters: [240]/[300] loss: 0.13\n",
      "Training DNN model......\n",
      "RNN iters: [250]/[300] loss: 0.16\n",
      "RNN iters: [260]/[300] loss: 0.14\n",
      "Accuracy of the network on the digits dataset: 90.67 %\n",
      "AUC of the network on the digits dataset: 0.99\n",
      "Training DNN model......\n",
      "RNN iters: [270]/[300] loss: 0.07\n",
      "RNN iters: [280]/[300] loss: 0.08\n",
      "Training DNN model......\n",
      "RNN iters: [290]/[300] loss: 0.10\n",
      "RNN iters: [300]/[300] loss: 0.10\n"
     ]
    }
   ],
   "source": [
    "# train RNN without tree regularization\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(num_retrains):\n",
    "    # train RNN model\n",
    "    print('Training DNN model......')\n",
    "    model.train()\n",
    "    for j in range(iters_retrain):\n",
    "        trn_x, trn_y = get_jth_minibatach(j, batch_size, X_train, y_train)\n",
    "        trn_x = trn_x.view(-1, sequence_length, input_size)\n",
    "        trn_x = trn_x.to(device)\n",
    "        trn_y = trn_y.to(device)\n",
    "        output = model(trn_x)\n",
    "        loss = criterion(output, trn_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i*iters_retrain + j + 1) % 10 == 0:\n",
    "            print('RNN iters: [{0}]/[{1}] loss: {2:.2f}'.format((i*iters_retrain + j + 1), num_iters, loss.item()))\n",
    "    if i % 3 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            X_test = X_test.view(-1, sequence_length, input_size)\n",
    "            X_test = X_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            outputs = model(X_test)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_test.size(0)\n",
    "            correct += (predicted == y_test).sum().item()\n",
    "            y_score = F.softmax(outputs, dim=1)\n",
    "\n",
    "            print('Accuracy of the network on the digits dataset: {0:.2f} %'.format(100 * correct / total))\n",
    "            print('AUC of the network on the digits dataset: {0:.2f}'.format(roc_auc_score(y_test.cpu().numpy(), \n",
    "                                                                                           y_score.cpu().numpy(),\n",
    "                                                                                           multi_class='ovo')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN model......\n",
      "RNN iters: [10]/[300] loss: 1.95 Estimated APL: -0.13\n",
      "RNN iters: [20]/[300] loss: 1.61 Estimated APL: -0.15\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 6.55\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.65\n",
      "Surrogate iters: [400]/[1000] loss: 0.52\n",
      "Surrogate iters: [600]/[1000] loss: 0.42\n",
      "Surrogate iters: [800]/[1000] loss: 0.35\n",
      "Surrogate iters: [1000]/[1000] loss: 0.31\n",
      "Accuracy of the network on the digits dataset: 58.22 %\n",
      "AUC of the network on the digits dataset: 0.92\n",
      "Training RNN model......\n",
      "RNN iters: [30]/[300] loss: 0.08 Estimated APL: -12.52\n",
      "RNN iters: [40]/[300] loss: -1.85 Estimated APL: -29.58\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 6.57\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.34\n",
      "Surrogate iters: [400]/[1000] loss: 0.32\n",
      "Surrogate iters: [600]/[1000] loss: 0.30\n",
      "Surrogate iters: [800]/[1000] loss: 0.28\n",
      "Surrogate iters: [1000]/[1000] loss: 0.26\n",
      "Training RNN model......\n",
      "RNN iters: [50]/[300] loss: 1.65 Estimated APL: 6.32\n",
      "RNN iters: [60]/[300] loss: 1.25 Estimated APL: 5.04\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 6.57\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.34\n",
      "Surrogate iters: [400]/[1000] loss: 0.27\n",
      "Surrogate iters: [600]/[1000] loss: 0.25\n",
      "Surrogate iters: [800]/[1000] loss: 0.23\n",
      "Surrogate iters: [1000]/[1000] loss: 0.22\n",
      "Training RNN model......\n",
      "RNN iters: [70]/[300] loss: 1.38 Estimated APL: 5.50\n",
      "RNN iters: [80]/[300] loss: 0.97 Estimated APL: 3.40\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 6.63\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.23\n",
      "Surrogate iters: [400]/[1000] loss: 0.58\n",
      "Surrogate iters: [600]/[1000] loss: 0.20\n",
      "Surrogate iters: [800]/[1000] loss: 0.19\n",
      "Surrogate iters: [1000]/[1000] loss: 0.55\n",
      "Accuracy of the network on the digits dataset: 78.22 %\n",
      "AUC of the network on the digits dataset: 0.97\n",
      "Training RNN model......\n",
      "RNN iters: [90]/[300] loss: 1.35 Estimated APL: 6.44\n",
      "RNN iters: [100]/[300] loss: 1.09 Estimated APL: 5.26\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 6.72\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.21\n",
      "Surrogate iters: [400]/[1000] loss: 0.18\n",
      "Surrogate iters: [600]/[1000] loss: 0.16\n",
      "Surrogate iters: [800]/[1000] loss: 0.14\n",
      "Surrogate iters: [1000]/[1000] loss: 0.15\n",
      "Training RNN model......\n",
      "RNN iters: [110]/[300] loss: 1.33 Estimated APL: 7.04\n",
      "RNN iters: [120]/[300] loss: 1.14 Estimated APL: 6.28\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 6.95\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.15\n",
      "Surrogate iters: [400]/[1000] loss: 0.13\n",
      "Surrogate iters: [600]/[1000] loss: 0.11\n",
      "Surrogate iters: [800]/[1000] loss: 0.10\n",
      "Surrogate iters: [1000]/[1000] loss: 0.08\n",
      "Training RNN model......\n",
      "RNN iters: [130]/[300] loss: 1.27 Estimated APL: 7.09\n",
      "RNN iters: [140]/[300] loss: 1.16 Estimated APL: 7.08\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 6.98\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.07\n",
      "Surrogate iters: [400]/[1000] loss: 0.07\n",
      "Surrogate iters: [600]/[1000] loss: 0.06\n",
      "Surrogate iters: [800]/[1000] loss: 0.06\n",
      "Surrogate iters: [1000]/[1000] loss: 0.06\n",
      "Accuracy of the network on the digits dataset: 85.33 %\n",
      "AUC of the network on the digits dataset: 0.98\n",
      "Training RNN model......\n",
      "RNN iters: [150]/[300] loss: 1.16 Estimated APL: 7.09\n",
      "RNN iters: [160]/[300] loss: 1.05 Estimated APL: 7.00\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 7.00\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.04\n",
      "Surrogate iters: [400]/[1000] loss: 0.04\n",
      "Surrogate iters: [600]/[1000] loss: 0.04\n",
      "Surrogate iters: [800]/[1000] loss: 0.06\n",
      "Surrogate iters: [1000]/[1000] loss: 0.04\n",
      "Training RNN model......\n",
      "RNN iters: [170]/[300] loss: 1.08 Estimated APL: 6.91\n",
      "RNN iters: [180]/[300] loss: 0.98 Estimated APL: 6.82\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 7.03\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.06\n",
      "Surrogate iters: [400]/[1000] loss: 0.06\n",
      "Surrogate iters: [600]/[1000] loss: 0.05\n",
      "Surrogate iters: [800]/[1000] loss: 0.05\n",
      "Surrogate iters: [1000]/[1000] loss: 0.05\n",
      "Training RNN model......\n",
      "RNN iters: [190]/[300] loss: 1.04 Estimated APL: 7.11\n",
      "RNN iters: [200]/[300] loss: 0.98 Estimated APL: 7.09\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 7.06\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.08\n",
      "Surrogate iters: [400]/[1000] loss: 0.07\n",
      "Surrogate iters: [600]/[1000] loss: 0.06\n",
      "Surrogate iters: [800]/[1000] loss: 0.07\n",
      "Surrogate iters: [1000]/[1000] loss: 0.07\n",
      "Accuracy of the network on the digits dataset: 89.78 %\n",
      "AUC of the network on the digits dataset: 0.99\n",
      "Training RNN model......\n",
      "RNN iters: [210]/[300] loss: 1.02 Estimated APL: 7.31\n",
      "RNN iters: [220]/[300] loss: 0.98 Estimated APL: 7.32\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 7.34\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.10\n",
      "Surrogate iters: [400]/[1000] loss: 0.09\n",
      "Surrogate iters: [600]/[1000] loss: 0.09\n",
      "Surrogate iters: [800]/[1000] loss: 0.09\n",
      "Surrogate iters: [1000]/[1000] loss: 0.08\n",
      "Training RNN model......\n",
      "RNN iters: [230]/[300] loss: 0.96 Estimated APL: 6.89\n",
      "RNN iters: [240]/[300] loss: 0.81 Estimated APL: 6.01\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 7.35\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.04\n",
      "Surrogate iters: [400]/[1000] loss: 0.04\n",
      "Surrogate iters: [600]/[1000] loss: 0.04\n",
      "Surrogate iters: [800]/[1000] loss: 0.03\n",
      "Surrogate iters: [1000]/[1000] loss: 0.03\n",
      "Training RNN model......\n",
      "RNN iters: [250]/[300] loss: 0.99 Estimated APL: 7.42\n",
      "RNN iters: [260]/[300] loss: 0.90 Estimated APL: 7.35\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 7.35\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.07\n",
      "Surrogate iters: [400]/[1000] loss: 0.06\n",
      "Surrogate iters: [600]/[1000] loss: 0.06\n",
      "Surrogate iters: [800]/[1000] loss: 0.06\n",
      "Surrogate iters: [1000]/[1000] loss: 0.06\n",
      "Accuracy of the network on the digits dataset: 91.11 %\n",
      "AUC of the network on the digits dataset: 0.99\n",
      "Training RNN model......\n",
      "RNN iters: [270]/[300] loss: 0.94 Estimated APL: 7.18\n",
      "RNN iters: [280]/[300] loss: 0.88 Estimated APL: 7.07\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 7.34\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.08\n",
      "Surrogate iters: [400]/[1000] loss: 0.10\n",
      "Surrogate iters: [600]/[1000] loss: 0.07\n",
      "Surrogate iters: [800]/[1000] loss: 0.07\n",
      "Surrogate iters: [1000]/[1000] loss: 0.09\n",
      "Training RNN model......\n",
      "RNN iters: [290]/[300] loss: 0.92 Estimated APL: 7.13\n",
      "RNN iters: [300]/[300] loss: 0.94 Estimated APL: 7.01\n",
      "Get {weights, APL} dataset......\n",
      "Mean APL: 7.34\n",
      "Training surrogate model......\n",
      "Surrogate iters: [200]/[1000] loss: 0.05\n",
      "Surrogate iters: [400]/[1000] loss: 0.05\n",
      "Surrogate iters: [600]/[1000] loss: 0.04\n",
      "Surrogate iters: [800]/[1000] loss: 0.04\n",
      "Surrogate iters: [1000]/[1000] loss: 0.04\n"
     ]
    }
   ],
   "source": [
    "# train RNN with tree regularization\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# surrogate model\n",
    "surrogate_model = SurrogateModel()\n",
    "surrogate_model.to(device)\n",
    "criterion_surrogate = nn.MSELoss()\n",
    "optimizer_surrogate = optim.Adam(surrogate_model.parameters(), lr=learning_rate)\n",
    "for i in range(num_retrains):\n",
    "    if i == 0 or i % 5 == 0:\n",
    "        saved_model_state_dict = [] # save the model state dict in each iters_retrain\n",
    "    # train RNN model\n",
    "    print('Training RNN model......')\n",
    "    model.train()\n",
    "    for j in range(iters_retrain):\n",
    "        trn_x, trn_y = get_jth_minibatach(j, batch_size, X_train, y_train)\n",
    "        trn_x = trn_x.view(-1, sequence_length, input_size)\n",
    "        trn_x = trn_x.to(device)\n",
    "        trn_y = trn_y.to(device)\n",
    "        output = model(trn_x)\n",
    "        path_length = surrogate_model(model.named_parameters())\n",
    "        if i == 0:\n",
    "            loss = criterion(output, trn_y)\n",
    "        else:\n",
    "            loss = criterion(output, trn_y) + lambda_punish * path_length\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        saved_model_state_dict.append(copy.deepcopy(model.state_dict()))\n",
    "        if (i*iters_retrain + j + 1) % 10 == 0:\n",
    "            print('RNN iters: [{0}]/[{1}] loss: {2:.2f} Estimated APL: {3:.2f}'.format((i*iters_retrain + j + 1), num_iters, \n",
    "                                                                                        loss.item(), path_length.item()))\n",
    "    # train Decision Tree to get {weights, APL} dataset\n",
    "    print('Get {weights, APL} dataset......')\n",
    "    y_APL_train = get_y_APL_train(saved_model_state_dict, X_train)\n",
    "    print('Mean APL: {0:.2f}'.format(y_APL_train.mean().item()))\n",
    "    print('Training surrogate model......')\n",
    "    # train surrogate model\n",
    "    for j in range(1000):\n",
    "        trn_x, trn_y = get_jth_minibatach(j, batch_size, saved_model_state_dict, y_APL_train)\n",
    "        trn_y = trn_y.to(device)\n",
    "        output = torch.zeros(trn_y.size(0), device=device)\n",
    "        for k in range(len(trn_x)):\n",
    "            output[k] = surrogate_model(trn_x[k].items())\n",
    "        loss = criterion_surrogate(output, trn_y)\n",
    "        # l2 norm\n",
    "        l2_norm = 0\n",
    "        for key, value in surrogate_model.named_parameters():\n",
    "            if key.endswith('weight'):\n",
    "                l2_norm += value.norm()\n",
    "        loss += epsilon_punish * l2_norm\n",
    "        optimizer_surrogate.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_surrogate.step()\n",
    "        if (j+1) % 200 == 0:\n",
    "            print('Surrogate iters: [{0}]/[1000] loss: {1:.2f}'.format(j+1, loss.item()))\n",
    "    if i % 3 == 0:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            X_test = X_test.view(-1, sequence_length, input_size)\n",
    "            X_test = X_test.to(device)\n",
    "            y_test = y_test.to(device)\n",
    "            outputs = model(X_test)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_test.size(0)\n",
    "            correct += (predicted == y_test).sum().item()\n",
    "            y_score = F.softmax(outputs, dim=1)\n",
    "\n",
    "            print('Accuracy of the network on the digits dataset: {0:.2f} %'.format(100 * correct / total))\n",
    "            print('AUC of the network on the digits dataset: {0:.2f}'.format(roc_auc_score(y_test.cpu().numpy(), \n",
    "                                                                                           y_score.cpu().numpy(),\n",
    "                                                                                           multi_class='ovo')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the digits dataset: 92.00 %\n",
      "AUC of the network on the digits dataset: 0.99\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    X_test = X_test.view(-1, sequence_length, input_size)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += y_test.size(0)\n",
    "    correct += (predicted == y_test).sum().item()\n",
    "    y_score = F.softmax(outputs, dim=1)\n",
    "\n",
    "    print('Accuracy of the network on the digits dataset: {0:.2f} %'.format(100 * correct / total))\n",
    "    print('AUC of the network on the digits dataset: {0:.2f}'.format(roc_auc_score(y_test.cpu().numpy(), \n",
    "                                                                                   y_score.cpu().numpy(),\n",
    "                                                                                   multi_class='ovo')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './models/gru_model_' + str(lambda_punish) + '.pth')\n",
    "torch.save(surrogate_model.state_dict(), './models/gru_surrogate_model_' + str(lambda_punish) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c8920ebee199>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m dot_data = export_graphviz(tree, out_file=None,\n\u001b[0;32m     16\u001b[0m                            \u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    425\u001b[0m         \"\"\"\n\u001b[0;32m    426\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m         \u001b[0mproba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[0;32m    390\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[1;32m--> 641\u001b[1;33m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "# visualize\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('./models/gru_model_' + str(lambda_punish) + '.pth', map_location=torch.device('cpu')))\n",
    "X_train = X_train.to(device)\n",
    "X_train = X_train.view(-1, sequence_length, input_size)\n",
    "outputs = model(X_train)\n",
    "_, y_pred = torch.max(outputs.data, 1)\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=25)\n",
    "X_train = X_train.view(y_pred.size(0), -1)\n",
    "X_train = X_train.to(torch.device('cpu'))\n",
    "y_pred = y_pred.to(torch.device('cpu'))\n",
    "tree.fit(X_train.numpy(), y_pred.numpy())\n",
    "print(accuracy_score(y_test.cpu().numpy(), tree.predict(X_test.cpu().numpy())))\n",
    "dot_data = export_graphviz(tree, out_file=None,\n",
    "                           feature_names=data.feature_names,\n",
    "                           class_names=[str(name) for name in data.target_names],\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf('./visualize/tree_on_gru_regularization_visualize_' + str(lambda_punish) + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the decision tree on original dataset: 80.22 %\n",
      "AUC of the decision tree on original dataset: 0.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize tree trained on original dataset\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=25)\n",
    "X_train = X_train.to(torch.device('cpu'))\n",
    "y_train = y_train.to(torch.device('cpu'))\n",
    "tree.fit(X_train.numpy(), y_train.numpy())\n",
    "y_pred = tree.predict(X_test)\n",
    "y_score = tree.predict_proba(X_test)\n",
    "print('Accuracy of the decision tree on original dataset: {0:.2f} %'.format(accuracy_score(y_test, y_pred)*100))\n",
    "print('AUC of the decision tree on original dataset: {0:.2f}'.format(roc_auc_score(y_test, y_score, multi_class='ovo')))\n",
    "dot_data = export_graphviz(tree, out_file=None,\n",
    "                           feature_names=data.feature_names,\n",
    "                           class_names=[str(name) for name in data.target_names],\n",
    "                           filled=True, rounded=True,\n",
    "                           special_characters=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf('./visualize/decision_tree_on_original_digits_dataset_visualize.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
